{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-laI3QiTYEY1",
        "AyAa1yfHP0MD",
        "X-1YqFjRQBFb",
        "ZvCUJnhLQy7p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Key Notes"
      ],
      "metadata": {
        "id": "-laI3QiTYEY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A - What are the things we do in text preprocessing?**\n",
        "\n",
        "\n",
        "1.   Convert text in lower cases\n",
        "2.   Remove numbers, special characters and punctuations, fullstops commas\n",
        "3.   Tokenisation\n",
        "4.   Stemming and lemitisation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LLBu_SfACOcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B - Then we convert these text into vectors (numbers) which is called embeddings.**\n"
      ],
      "metadata": {
        "id": "o997xruaChgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C- Convert these vectors to train and test and feed this test data into models.**"
      ],
      "metadata": {
        "id": "kTiTxfYPCx0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D - Calculate the cost to evaluate performance of the model.**"
      ],
      "metadata": {
        "id": "xPgodbLjC07a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Set Link**\n",
        "\n",
        "[IMDB DataSet](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/download?datasetVersionNumber=1)"
      ],
      "metadata": {
        "id": "JjrB8rZ_C5rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "ARbzClE-YK2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libaries**"
      ],
      "metadata": {
        "id": "mxmmCjZQDGOR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zdEObqTBnaV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import nltk\n",
        "import math\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dataset"
      ],
      "metadata": {
        "id": "G8tZ4c0FCfdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv')"
      ],
      "metadata": {
        "id": "7mNmrFX-CHvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(to_replace=\"positive\",\n",
        "           value=\"1\", inplace = True)\n",
        "df.replace(to_replace=\"negative\",\n",
        "           value=\"0\", inplace = True)"
      ],
      "metadata": {
        "id": "-r_orMWmYZQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.iloc[:5000,:]"
      ],
      "metadata": {
        "id": "_bkf0KB5mP77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "P1cul-wADYfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "-kLhTQ6ABs2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "KKu1JDrKse1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing :\n"
      ],
      "metadata": {
        "id": "zDoTfVKhCNDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer"
      ],
      "metadata": {
        "id": "tUOsC-C1TuDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "k1K_mrS8T1c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "uK_EV6r_UG2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ToktokTokenizer()"
      ],
      "metadata": {
        "id": "dpEyr4ePUXbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "eiFzS_t4Ue_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "cvH7FGGXUsBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(strip_html)\n",
        "\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(remove_between_square_brackets)\n",
        "\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(remove_special_characters)\n",
        "\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(denoise_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "2-VZqMcXVMzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #Tokenize\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "\n",
        "  #Removing blanks\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "\n",
        "  #Removing Stopwords\n",
        "  new_tokens = [token for token in tokens if token not in en_stopwords]\n",
        "\n",
        "  #Stemming\n",
        "  stemmed_token = [ps.lemmatize(token) for token in new_tokens]\n",
        "\n",
        "  clean_text = \" \".join(stemmed_token)\n",
        "\n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "MZoJPloIC4o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(cleanText)\n",
        "\n",
        "#lst = [cleanText(i) for i in df['review']]"
      ],
      "metadata": {
        "id": "1kGWQPdPWQiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review']"
      ],
      "metadata": {
        "id": "-uq1jGszCEPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "pWBjvrUPXqBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Denoted as bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "h8rt38R2XjXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer\n",
        "cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "\n",
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))"
      ],
      "metadata": {
        "id": "DwXaxhMTX1Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.iloc[:,1:]"
      ],
      "metadata": {
        "id": "wbC-GKCBbCsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,:-1]"
      ],
      "metadata": {
        "id": "mOIsQ5D5FC5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test =  train_test_split(x.values, y.values, test_size = 0.40, random_state = 5)"
      ],
      "metadata": {
        "id": "TFNbhQI-bE9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer\n",
        "cv_x_train = cv.fit_transform(x_train.ravel())\n",
        "\n",
        "#transformed train reviews\n",
        "tv_x_train =tv.fit_transform(x_train.ravel())\n"
      ],
      "metadata": {
        "id": "bo_2BJ3Dsput"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorizer\n",
        "cv_x_test = cv.transform(x_test.ravel())\n",
        "\n",
        "#transformed test reviews\n",
        "tv_x_test=tv.transform(x_test.ravel())\n"
      ],
      "metadata": {
        "id": "CKcgitTIstux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tfidf_train:',tv_x_train.shape)\n",
        "print('Tfidf_test:',tv_x_test.shape)"
      ],
      "metadata": {
        "id": "vW_HHgYJJNL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "#labeling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "\n",
        "#transformed sentiment data\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "WK_vScJBEU4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_train = y_train.values.ravel() # To convert y in 1-D Vector\n",
        "#y_test = y_test.values.ravel()"
      ],
      "metadata": {
        "id": "c1bV0875jLRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "id": "z4BrR6WWdOu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "Gd3zboE2gVGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "DxrGjLX8d-P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = MultinomialNB()"
      ],
      "metadata": {
        "id": "ld5MJ93ZgTTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_nb = nb.fit(cv_x_train,y_train)"
      ],
      "metadata": {
        "id": "1IQzHxFUgqYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_nb = nb.fit(tv_x_train,y_train)"
      ],
      "metadata": {
        "id": "UswL_QQ9JoT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_y_pred = cv_nb.predict(cv_x_test)"
      ],
      "metadata": {
        "id": "T_B3wL4GguYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_y_pred = tv_nb.predict(tv_x_test)"
      ],
      "metadata": {
        "id": "H29NiTy9Jttf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cost\n",
        "cv_cost = -np.sum(np.log(nb.predict_proba(cv_x_test)))\n",
        "tv_cost = -np.sum(np.log(nb.predict_proba(cv_x_test)))"
      ],
      "metadata": {
        "id": "tT41Ht-BnGNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_cost, tv_cost"
      ],
      "metadata": {
        "id": "TU10-vlTnQ3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Accuracy for Multinomial Naive Byes"
      ],
      "metadata": {
        "id": "xJAaz_AfNlIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy score for bag of words\n",
        "cv_nb_score = accuracy_score(y_test, cv_y_pred)\n",
        "print(\"CountVectorization score for MNB :\",cv_nb_score)\n",
        "\n",
        "#Accuracy score for tfidf features\n",
        "tv_nb_score = accuracy_score(y_test, tv_y_pred)\n",
        "print(\"TFID Vectorization score for MNB :\",tv_nb_score)"
      ],
      "metadata": {
        "id": "yIPvWUBBNkrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Model by real time data"
      ],
      "metadata": {
        "id": "AyAa1yfHP0MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check = ['This movie was really bad', 'Wow what a movie', 'I think I prefer watching some other movie']"
      ],
      "metadata": {
        "id": "aqg76Q_foWoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_check = []\n",
        "for i in check:\n",
        "  new_check.append(cleanText(i))"
      ],
      "metadata": {
        "id": "LZfsXvcxou0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_check_test = cv.transform(new_check)\n",
        "tv_check_test = tv.transform(new_check)"
      ],
      "metadata": {
        "id": "Eg46Q4RmpArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_new_y_pred = nb.predict(cv_check_test)\n",
        "tv_new_y_pred = nb.predict(tv_check_test)"
      ],
      "metadata": {
        "id": "4iuvbgdapTm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_new_y_pred, tv_new_y_pred"
      ],
      "metadata": {
        "id": "lRgF02krpX4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report for mnb -"
      ],
      "metadata": {
        "id": "X-1YqFjRQBFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification report for bag of words\n",
        "mnb_cv_report = classification_report(y_test, cv_y_pred,target_names=['Positive','Negative'])\n",
        "print(mnb_cv_report)\n"
      ],
      "metadata": {
        "id": "z09G5LoMP-Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification report for tfidf features\n",
        "mnb_tv_report = classification_report(y_test, tv_y_pred,target_names=['Positive','Negative'])\n",
        "print(mnb_tv_report)"
      ],
      "metadata": {
        "id": "vxI23yWvQbKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix for mnb"
      ],
      "metadata": {
        "id": "ZvCUJnhLQy7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_cv = confusion_matrix(y_test, cv_y_pred,labels=[1,0])\n",
        "print(cm_cv)"
      ],
      "metadata": {
        "id": "y97jeylaQvY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix for tfidf features\n",
        "cm_tfidf = confusion_matrix(y_test, tv_y_pred,labels=[1,0])\n",
        "print(cm_tfidf)"
      ],
      "metadata": {
        "id": "o2lZXKZbQ6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordCloud for mnb"
      ],
      "metadata": {
        "id": "TrMacjzqRRrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word cloud for positive review words"
      ],
      "metadata": {
        "id": "6gqhagoPRVYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "cp41LUkMRqC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1][0]"
      ],
      "metadata": {
        "id": "EFU0DrLWR1T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word cloud for positive review words\n",
        "plt.figure(figsize=(10,10))\n",
        "positive_text=x_train[4][0]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "positive_words=WC.generate(positive_text)\n",
        "plt.imshow(positive_words,interpolation='bilinear')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "_lRBunLvRRZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word cloud for negative review words"
      ],
      "metadata": {
        "id": "5yXzJ-j8SOj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Word cloud for negative review words\n",
        "plt.figure(figsize=(10,10))\n",
        "negative_text=x_train[2][0]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "negative_words=WC.generate(negative_text)\n",
        "plt.imshow(negative_words,interpolation='bilinear')\n",
        "plt.show"
      ],
      "metadata": {
        "id": "8nRtghsvSQWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector Classifier"
      ],
      "metadata": {
        "id": "V4DgHV-MTpFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "JxvCiLGVUBAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = SVC(kernel = 'rbf')"
      ],
      "metadata": {
        "id": "RsACZQIHUFtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_svr = regressor.fit(cv_x_train,y_train)"
      ],
      "metadata": {
        "id": "kNPVqMWwUpYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_svr = regressor.fit(tv_x_train,y_train)"
      ],
      "metadata": {
        "id": "v8Kzuk8NVASA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svr_cv_y_pred = cv_svr.predict(cv_x_test)\n",
        "svr_tv_y_pred = tv_svr.predict(tv_x_test)"
      ],
      "metadata": {
        "id": "UICbo27xVDf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svr_cv_y_pred"
      ],
      "metadata": {
        "id": "d3mWxXnxVuqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Accuracy for Support Vector Classifier"
      ],
      "metadata": {
        "id": "lNw9O5ipVdL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy score for bag of words\n",
        "cv_svr_score = accuracy_score(y_test, svr_cv_y_pred)\n",
        "print(\"CountVectorization score for MNB :\",cv_svr_score)\n",
        "\n",
        "#Accuracy score for tfidf features\n",
        "tv_svr_score = accuracy_score(y_test, svr_tv_y_pred)\n",
        "print(\"TFID Vectorization score for MNB :\",tv_svr_score)"
      ],
      "metadata": {
        "id": "fjPLtfgQVfvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "OC-Kjeo9WVAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression,SGDClassifier"
      ],
      "metadata": {
        "id": "IhVOjaR3Vodn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)"
      ],
      "metadata": {
        "id": "qfJOy-Q_WeR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_x_train,y_train)\n",
        "print(lr_bow)\n",
        "\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_x_train,y_train)\n",
        "print(lr_tfidf)"
      ],
      "metadata": {
        "id": "rw1U9bQAWuiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_cv_y_pred = lr_bow.predict(cv_x_test)\n",
        "lr_tv_y_pred = lr_tfidf.predict(tv_x_test)"
      ],
      "metadata": {
        "id": "m_FJyXQeXMXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_tv_y_pred"
      ],
      "metadata": {
        "id": "J4-XuD3IXRjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Accuracy for Logistic Regression\n"
      ],
      "metadata": {
        "id": "18WzPw7oXXPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy score for bag of words\n",
        "cv_lr_score = accuracy_score(y_test, lr_cv_y_pred)\n",
        "print(\"CountVectorization score for MNB :\",cv_lr_score)\n",
        "\n",
        "#Accuracy score for tfidf features\n",
        "tv_lr_score = accuracy_score(y_test, lr_tv_y_pred)\n",
        "print(\"TFID Vectorization score for MNB :\",tv_lr_score)"
      ],
      "metadata": {
        "id": "1YIxWn1dXUia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observed that both logistic regression and multinomial naive bayes model performing well compared to support vector machines."
      ],
      "metadata": {
        "id": "6cUBy8I6X84b"
      }
    }
  ]
}